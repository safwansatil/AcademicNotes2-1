## Some Definitions

**Latency:** The time to complete a single task from start to finish.

**Throughput:** The number of tasks completed per unit of time.

**Computer Architecture** involves designing the instruction set architecture (ISA), which defines the instructions that a processor can execute, as well as the memory hierarchy, system interconnects, parallel processing techniques, and performance optimization strategies.

**Basic Organization of a Computer:** 5 parts: Input, Output, Memory, ALU, Control Unit.

### Moore's Law

The number of transistors on a microchip doubles about every two years, though the cost of computers is halved. This trend has continued for several decades, leading to exponential growth in computing power.

**Threats to Moore's Law:** Physical limitations, heat dissipation, manufacturing costs, energy efficiency, etc.

### Other Exponential Changes

1. **Computational Capacity:** The computational capacity of computers has increased exponentially, doubling every 1.5 years
2. **Computing Efficiency:** Measuring the energy use of computers – has halved every 1.5 years over the last 60 years. This means that for the same amount of computational work, computers now use half the energy compared to just 1.5 years ago.

## 8 Great Ideas in Computer Architecture

1. **Design for Moore's Law:** Architects must anticipate where the technology will be when the design finishes rather than design for where it starts

2. **Use Abstraction to Simplify Design:** Use abstractions to represent the design at different levels of representation. This is necessary to keep up with the dramatic growth of resources defined by Moore's Law

3. **Make Common Case Fast:** You get the biggest performance bang for your buck by focusing your optimization efforts on the things that happen most often. That's what "making the common case fast" means, and it's why modern computers feel so responsive

4. **Parallelism:** Exploit parallelism to improve performance. This can be done at various levels, from instruction-level parallelism within a single processor to data-level and task-level parallelism across multiple processors

5. **Pipelining:** Pipelining involves breaking down the execution of instructions into stages and overlapping them to improve throughput

6. **Prediction:** In some cases it can be faster on average to guess and start working rather than wait until you know for sure, assuming that the mechanism to recover from a misprediction is not too expensive and your prediction is relatively accurate

7. **Memory Hierarchy:** This principle advocates the use of multiple levels of memory hierarchy, including registers, caches, and main memory, to provide a trade-off between speed and capacity. In other words, Fast, Large, and Cheap cannot all be together at the same time—only 2 can be selected

8. **Dependability via Redundancy:** Making the system resilient to failure. It's about reliability and availability (as opposed to optimizing storage hierarchy which is about performance and speed)

## Software Application

**Software Application:** Software that provides services that are commonly useful, including operating systems, compilers, loaders, and assemblers.

**Operating System:** Interfaces between a user's program and the hardware and provides a variety of services and supervisory functions such as:

- Handling input and output
- Allocating storage and memory
- Providing for protected sharing of the computer among multiple applications

## Power and Performance

**Power Required per Transistor:**
$$\text{Power} \propto \frac{1}{2} \times \text{capacitance} \times (\text{voltage})^2 \times \text{frequency}$$

**Challenges in Lowering Voltage:**

- Current leak
- Heat dissipation requiring expensive cooling solutions

**Power Wall:** A point beyond which it becomes extremely difficult to increase the clock speed or overall performance of a processor due to the constraints imposed by power consumption and heat dissipation

## The Switch from Uniprocessors to Multiprocessors

**Benefits:**

- Improved performance
- Increased throughput
- Energy efficiency
- Fault tolerance

To utilize the best of these advantages, programmers must design or rewrite codes that effectively use multiprocessors. They will have to continue to improve performance of their code as the number of cores increases.

In other words, you need to design your code with multiple threads or processes that can run in parallel on different cores. This involves learning about:

- Threading libraries (e.g., Python's threading, Java's java.util.concurrent)
- Parallel programming models (e.g., OpenMP, MPI)
- Asynchronous programming (e.g., async/await in C#/Python/JS)

### New Challenges and Bugs

Parallel programming is hard. You introduce new, subtle bugs that don't exist in sequential code:

**Race Conditions:** Two chefs trying to use the last egg at the same time.

**Deadlocks:** Chef A is waiting for Chef B's knife, and Chef B is waiting for Chef A's bowl. Both are stuck forever.

### The Scalability Challenge

The sentence "They will have to continue to improve performance... as the number of cores increases" is crucial. Writing code that uses 4 cores is one thing. Writing code that efficiently uses 64 or 128 cores is a much harder problem, as the overhead of coordination between cores can start to dominate.

**In a nutshell:** The hardware revolution has moved from making faster chefs to giving us more chefs. The burden is now on us, the software engineers, to write the "recipes" (code) that can keep all those chefs busy at the same time. This is the defining challenge of modern software performance.

## Fallacies and Pitfalls

### Fallacy 1: Computers at Low Utilization Use Low Power

Even when idle, many components consume significant power.

**Energy Waste:** It creates a huge incentive to consolidate workloads. Instead of having 10 servers running at 10% load (each wasting power on fixed overhead), it's far more efficient to have 1 server running at 100% load and turn the other 9 off. This is the driving idea behind cloud computing and virtualization.

### Fallacy 2: Performance and Energy Efficiency Are Unrelated Goals

Hardware or software optimizations that take less time save energy overall, even if the optimization takes a bit more energy when it is used.

**Race-to-Idle Strategy:** Get from the Active State to the Idle State as quickly as possible.

**Takeaway:** This principle flips a common intuition on its head. It's not always about minimizing instantaneous power draw. When optimizing for battery life or energy bills, sometimes the best solution is to throw more power at the problem to get it done faster.

This is why modern systems use aggressive "Turbo" modes and specialized hardware. The goal isn't just peak performance; it's also about getting back to a low-power state to save total energy over time.

### Pitfall 1: Amdahl's Law

**Amdahl's Law:** A rule stating that the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used.

**The Core Idea:** The Law of Diminishing Returns for Optimization

Amdahl's Law states a simple but powerful truth: The overall speedup of a system is limited by the part you did NOT improve.

Think of it like a chain: you can make some links incredibly strong, but the chain's overall strength is determined by its weakest link.

### Pitfall 2: Using a Subset of the Performance Equation as a Performance Metric

Using only one metric to measure performance—for example, using only clock speed—ignores other important factors like instructions per cycle (IPC) and the number of cores. This incomplete view can lead to misleading conclusions about system performance.

---

## Key Takeaways from Lecture 1

- **Architecture vs Organization:** CA designs what a CPU should do (ISA), while CO designs how to implement it efficiently
- **Moore's Law Limits:** Despite decades of exponential growth, we're hitting physical and thermal limits
- **From Speed to Scale:** The focus has shifted from making single-core processors faster to building multi-core systems and making software that can utilize them
- **Power Constraints:** Heat and power consumption are now the primary bottlenecks in performance improvement
- **Parallelism Burden:** Programmers must now learn parallel programming paradigms to achieve good performance
- **Energy-Performance Trade-off:** Sometimes using more power temporarily can lead to overall energy savings
